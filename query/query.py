"""
Query processor for MS MARCO dataset.
Loads index files, parses query, fetches postings, and returns matching docIDs.
"""

from os import path
from json import load
from typing import Dict, Set, List

def run_query(input_dir: str, query: str, conjunctive: bool = True) -> List[int]:
    """
    Run a simple query against the text-based inverted index.
    conjunctive=True => AND semantics
    conjunctive=False => OR semantics
    """
    lexicon: Dict[str, Dict] = load_lexicon(path.join(input_dir, "lexicon.json"))
    page_table: Dict[str, Dict] = load_page_table(path.join(input_dir, "page_table.json"))
    inverted_index: Dict[str, Dict[str, List[int]]] = load_inverted_index(path.join(input_dir, "inverted_index.bin"))

    # TEMP: simple whitespace tokenizer (unify with shared tokenizer later)
    terms: List[str] = query.lower().split()

    # Retrieve postings lists for all query terms
    postings_lists: List[List[int]] = [inverted_index[term]["docIDs"] for term in terms]

    # Initialize with the first postings list
    doc_ids: Set[int] = set(postings_lists[0])
    remaining_postings: List[List[int]] = postings_lists[1:]
    
    # Apply AND/OR semantics across postings lists
    if conjunctive:
        # Intersect all postings (documents must contain every term)
        for posting_list in remaining_postings:
            doc_ids &= set(posting_list)
    else:
        # Union all postings (documents may contain any term)
        for posting_list in remaining_postings:
            doc_ids |= set(posting_list)

    return sorted(doc_ids)

def load_lexicon(lexicon_path: str) -> Dict[str, Dict]:
    with open(lexicon_path, "r", encoding="utf-8") as lexicon_file:
        return load(lexicon_file)

def load_page_table(page_table_path: str) -> Dict[str, Dict]:
    with open(page_table_path, "r", encoding="utf-8") as page_table_file:
        return load(page_table_file)

def load_inverted_index(inverted_index_path: str) -> Dict[str, Dict[str, List[int]]]:
    """
    Load the plain-text inverted index generated by the indexer.
    Each line: 'term docID1:freq1 docID2:freq2 ...'
    """
    inverted_index: Dict[str, Dict[str, List[int]]] = {}

    with open(inverted_index_path, "r", encoding="utf-8") as inverted_index_file:
        for line in inverted_index_file:
            # Split line into term and postings segment
            tokens: List[str] = line.strip().split()
            term: str = tokens[0]
            postings: List[str] = tokens[1:]
            
            doc_ids: List[int] = []
            freqs: List[int] = []

            # Parse each 'docID:freq' pair
            for posting in postings:
                doc_id, freq = posting.split(":")
                doc_ids.append(int(doc_id))
                freqs.append(int(freq))

            # Store term entry in inverted index
            inverted_index[term] = {"docIDs": doc_ids, "freqs": freqs}

    return inverted_index